
Source
https://blog.keras.io/building-autoencoders-in-keras.html


=====================================================================

@Talk to Rao about your idea.

REVIEW: training with 8 and feeding other numbers causes you to see them as 8s !!
AND THAT maybe good. The degree of reconstruction match is an estimate of the match.

TRY EXPERIMENT: BUT low reward images
may get heightened if SAME features. Hence occlusion is necessary
TEST if low reward images get heightened if features overlap. THIS MAYBE ACCEPTABLE.

CHANGE:
If occlusion is goal, then train to BLACK ( 0). What is benefit of noise ? Let the reward be how much of the
image to KEEP.

IMPORTANT: HERE the reward is NOT feature based , but category based. So training on 7, will not recover 1, because
1 is penalized, so it learns to occlude if the horizontal bar "-" of 7 is missing.
EXPERIMENT: If 1 is trained at 0.5, and then 1 is trained at 0.5 WITH 7 at 1. Does it improve !!
ANSWER: No it get's worse ?? Why ? because those higher rewards are clearer and are higher value cuz easier to get right.
Wherehas 1, has NOISE that it needs to predict as well. Maybe


EXPERIMENT
CAN you feed a partly recovered image repeatedly , and see if the number is recovered from it ??
EASIER experiment.
Train on 4 and 7.


<write down for later>
A large latent space would just give better separation. the CNN AE already has a large capacity.

<COOL IDEA- worth trying write down for later>
A Resnet based AE filter !! aha ! it does not Autoencode but Filter. IT IS NOT just an AE, THE AE part is to extract features
rather than pixel level reproduction, but the latter is important too. The resnet approach just feeds the image at the other end, and
we learn what to occlude ?? OR we could learn when NOT to occlude !! i.e. when you see a "7" feature, turn off occlusion.
or occlude MORE when you see the wrong features.

WAY OF THINKING: In essense, we are translating the users preferences into a latent space, and matching that with the current
input. More the match, more the recover. Less the match, more the occlusion. principal components of preferences.
Explicable. IN FACT, the recovery can be seen as the closest match to the PREFERENCE.



Tensorboard failed with your loss function. My guess is your loss function gives an array for the loss, as opposed to a single value !!
Try changing the loss function to an type that returns a single value !!



1) First have the reward be 1, and see that it works as normal. THEN change the reward
to 0.5, and see that it should still work, but like reducing the learning rate,
1.5) THEN use +1/0 reward, which should work like optimizing only a subset of pictures
1.7) Finally try +1/-1 reward which should be interesting.


?? can you quick test your idea using MNIST. all the data that is 9, make it all white.
Step 2: make a few numbers (that share features) noisy !! degree of noise ~ degree of reward
Step 3: When the reward is high, pay attention to the error. When the reward is low,
encode less (less adjustment!!).
IMPORTANT, YAS !! Step 4: What about encoding the negative or low reward to all zeros !! SO lower the reward
MORE random zeros. Increase the probability of flipping a pixel to zero. apply TO VIZDOOM
AT ZERO REWARD. 50% probability randomly.
@ google attention detection in visual data! and maybe add in RL keyword.

1) And your Reward driven AE, and expected result (in VIZDOOM basic, the reconstruction is better
when the monster and human are aligned !!)

2) in VIZDOOM basic, the reconstruction is better
when the monster and human are aligned !!)

4) Surveillance ( explicable exploration)  See your old hand notes. Say not really interested.
Will continue looking for a related angle.